{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBowC3JUyXcy",
    "outputId": "edb3d59c-4515-42e5-ae64-83fd081e2fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: telethon in /opt/miniconda3/lib/python3.12/site-packages (1.40.0)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: langdetect in /opt/miniconda3/lib/python3.12/site-packages (1.0.9)\n",
      "Requirement already satisfied: pyaes in /opt/miniconda3/lib/python3.12/site-packages (from telethon) (1.6.1)\n",
      "Requirement already satisfied: rsa in /opt/miniconda3/lib/python3.12/site-packages (from telethon) (4.9)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: six in /opt/miniconda3/lib/python3.12/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/miniconda3/lib/python3.12/site-packages (from rsa->telethon) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install telethon pandas requests langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMPetDnuEL_a",
    "outputId": "bb13a890-d08c-4ba5-bcdc-9c2cd5ed6344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/miniconda3/lib/python3.12/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFIAy13PHt80",
    "outputId": "41d84001-1d7e-4a59-ab43-a68e3f86f5b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /opt/miniconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QXdS3FUDHvyt"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRnvbW8UdykJ",
    "outputId": "f192320a-f469-4640-b498-8b16e23a1324"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "except ModuleNotFoundError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Cf2Q2pXXC4ZM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, csv, time, re\n",
    "import asyncio, requests, pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urlparse\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed=0\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oGRmc41x_UlG"
   },
   "outputs": [],
   "source": [
    "from telethon import TelegramClient, errors\n",
    "from telethon.tl.functions.channels import JoinChannelRequest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OMcGFBPF_UoF"
   },
   "outputs": [],
   "source": [
    "from telethon.sync import TelegramClient\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ID  = 25745274                       \n",
    "API_HASH = '9652fe84b03043cdbbd0e1c36f25d5a9'\n",
    "PHONE    = '+8613585795327'\n",
    "\n",
    "\n",
    "#back up account\n",
    "# API_ID  = 24527187                       \n",
    "# API_HASH = '8a012769064c0218d779c9e23e45fdb0'\n",
    "# PHONE    = '+18573343025'\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"API_ID\"]   = str(API_ID)      # must be strings\n",
    "os.environ[\"API_HASH\"] = API_HASH\n",
    "os.environ[\"PHONE\"]    = PHONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_MchyQTGC8IQ"
   },
   "outputs": [],
   "source": [
    "# expects API_ID, API_HASH, and PHONE in the environment\n",
    "API_ID = os.getenv(\"API_ID\")\n",
    "API_HASH = os.getenv(\"API_HASH\")\n",
    "PHONE = os.getenv(\"PHONE\")\n",
    "if not all([API_ID, API_HASH, PHONE]):\n",
    "    raise RuntimeError(\"API_ID, API_HASH and PHONE environment variables must be set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Matches @handles that are at least 5 characters, digits or '_'\n",
    "USERNAME_RE  = re.compile(r'@([A-Za-z0-9_]{5,})')\n",
    "\n",
    "# You already used this for text-link harvesting\n",
    "TME_LINK_RE  = re.compile(r'(?:t\\.me/|@)([A-Za-z0-9_]{5,})')\n",
    "TMELINK_RE   = TME_LINK_RE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, asyncio\n",
    "from telethon.errors import FloodWaitError\n",
    "\n",
    "# ---------- simple username validation ----------\n",
    "VALID_UNAME = re.compile(r'^@[A-Za-z0-9_]{5,32}$')\n",
    "\n",
    "def looks_like_username(s: str) -> bool:\n",
    "    return bool(VALID_UNAME.match(s.strip()))\n",
    "\n",
    "# ---------- global cache ----------\n",
    "_entity_cache: dict[str, \"telethon.tl.types.User|Channel|Chat\"] = {}\n",
    "\n",
    "# ---------- safe resolver ----------\n",
    "async def safe_get_entity(client, handle: str):\n",
    "    \"\"\"\n",
    "    Resolve @username → Telethon entity, with cache and Flood-Wait handling.\n",
    "    Returns None if the handle is malformed, deleted/private, or would trigger\n",
    "    a Flood-Wait > 10 min.\n",
    "    \"\"\"\n",
    "    handle = handle.strip()\n",
    "\n",
    "    # reject obviously bad strings before touching the API\n",
    "    if not looks_like_username(handle):\n",
    "        return None\n",
    "\n",
    "    # cached hit\n",
    "    if handle in _entity_cache:\n",
    "        return _entity_cache[handle]\n",
    "\n",
    "    try:\n",
    "        # **direct call to Telegram once**\n",
    "        ent = await client.get_entity(handle)\n",
    "        _entity_cache[handle] = ent\n",
    "        return ent\n",
    "\n",
    "    except FloodWaitError as e:\n",
    "        if e.seconds > 600:                    # long wait → skip handle\n",
    "            print(\"Skip\", handle, \"— FloodWait\", e.seconds, \"s\")\n",
    "            return None\n",
    "        print(\"Sleeping\", e.seconds, \"s for\", handle)\n",
    "        await asyncio.sleep(e.seconds + 2)\n",
    "        # retry once after short wait\n",
    "        return await safe_get_entity(client, handle)\n",
    "\n",
    "    except Exception:\n",
    "        # invalid / deleted / private\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ykINbo9OxNpF"
   },
   "outputs": [],
   "source": [
    "PROGRESS_JSON = os.getenv('PROGRESS_JSON', '/content/drive/MyDrive/telegram/progress.json')\n",
    "RAW_CSV = os.getenv('RAW_CSV', '/content/drive/MyDrive/telegram/messages2.csv')\n",
    "CLEAN_CSV = os.getenv('CLEAN_CSV', '/content/drive/MyDrive/telegram/cleaned.csv')\n",
    "SESSION_NAME = os.getenv('SESSION_NAME', 'tg_session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFS_HOPS              = 4      # how many BFS layers (depth)\n",
    "BFS_CHANNEL_MSG_LIMIT = 250    # deeper scan for channels\n",
    "BFS_GROUP_MSG_LIMIT   = 20     # smaller scan for groups to avoid excessive overhead\n",
    "BFS_MIN_SUBS          = 0      # no minimum subscriber threshold\n",
    "BFS_LANG_SAMPLE       = 3000   # messages sampled to check language\n",
    "BFS_EN_RATIO          = 0.2    # 20% must be English\n",
    "BFS_FWD_MIN           = 0      # min forwards to count a channel\n",
    "BFS_INCLUDE_MENTIONS  = True   # parse @username / t.me links\n",
    "BFS_TOPIC_KEYWORDS    = []     # keywords to filter topics (not used if empty)\n",
    "BFS_TOPIC_RATIO       = 0      # ratio threshold for keyword matches\n",
    "BFS_TOPIC_SAMPLE      = 20     # how many messages sampled for topic ratio\n",
    "BFS_MAX_DORMANT_DAYS  = 1825   # ignore channels dormant > X days\n",
    "\n",
    "\n",
    "# Global set to track which user IDs we've already scanned (for participant bounce)\n",
    "scanned_user_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Throttling helper ==================================================\n",
    "import asyncio, time, random\n",
    "from telethon import errors as _tl_errors\n",
    "\n",
    "RATE_LIMIT = 1.2      # ≥ 1 s between any two RPCs – raise if you still flood\n",
    "JITTER     = 0.3      # add ± up to 0.3 s so calls look “human”\n",
    "_last_call = 0.0\n",
    "\n",
    "async def safe_api_call(coro, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Call ANY Telethon coroutine (or the `client` object itself) while\n",
    "    1⃣  spacing requests, and 2⃣  automatically respecting FloodWait.\n",
    "    \"\"\"\n",
    "    global _last_call\n",
    "    gap = RATE_LIMIT + random.uniform(-JITTER, JITTER)\n",
    "    now = time.time()\n",
    "    if now - _last_call < gap:\n",
    "        await asyncio.sleep(gap - (now - _last_call))\n",
    "    _last_call = time.time()\n",
    "\n",
    "    try:\n",
    "        return await coro(*args, **kwargs)\n",
    "    except _tl_errors.FloodWaitError as e:\n",
    "        wait = e.seconds + 2               # +2 s safety pad\n",
    "        print(f\"⚠️  FloodWait {e.seconds}s → sleeping {wait}s …\")\n",
    "        await asyncio.sleep(wait)\n",
    "        _last_call = time.time()\n",
    "        return await coro(*args, **kwargs)  # one retry is enough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- numeric-ID bulk-resolution globals ---------------------------\n",
    "pending_ids: dict[int, tuple[int, int]] = {}   # cid → (access_hash, parent_hop)\n",
    "RESOLVE_INTERVAL = 300      # resolve after this many queued ids\n",
    "MAX_PER_CALL     = 100      # ≤100 ids per GetChannelsRequest  (Telegram soft cap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throttle settings --------------------------------------------------------\n",
    "import random, asyncio\n",
    "BFS_PAUSE_SEC   = 0.8        # base delay after each 100-msg pull\n",
    "BFS_BATCH_SIZE  = 100        # pause every 100 messages\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "async def _batched_iter_messages(entity, *, limit=None):\n",
    "    \"\"\"\n",
    "    Iterator that keeps us well below Telegram’s\n",
    "    ≈10 GetHistory-calls per 30 s soft limit.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    async for msg in client.iter_messages(entity, limit=limit):\n",
    "        yield msg\n",
    "        i += 1\n",
    "        if i % BFS_BATCH_SIZE == 0:\n",
    "            await asyncio.sleep(BFS_PAUSE_SEC + random.random()*0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ba2Vaw21xPEj"
   },
   "outputs": [],
   "source": [
    "channels_df = pd.read_csv('Final_tele.csv')\n",
    "def _parse_subs(s):\n",
    "    s = str(s).replace('+', '').strip()\n",
    "    mult = 1\n",
    "    if s.lower().endswith('k'):\n",
    "        mult = 1000\n",
    "        s = s[:-1]\n",
    "    elif s.lower().endswith('m'):\n",
    "        mult = 1000000\n",
    "        s = s[:-1]\n",
    "    try:\n",
    "        return int(float(s) * mult)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "channels_df['subs_num'] = channels_df['Subscribers'].apply(_parse_subs)\n",
    "seed_usernames = channels_df['Username'].tolist()\n",
    "FOLLOWERS_DICT = channels_df.set_index('Username')['subs_num'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zIi_m0JbdyX5"
   },
   "outputs": [],
   "source": [
    "CHANNELS = seed_usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Bdu3oFp__UrI"
   },
   "outputs": [],
   "source": [
    "client=TelegramClient(SESSION_NAME,API_ID,API_HASH)\n",
    "def load_progress():\n",
    "  if os.path.exists(PROGRESS_JSON):\n",
    "    with open(PROGRESS_JSON,'r')as f:return json.load(f)\n",
    "  return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XlaWF6Z0AY6-"
   },
   "outputs": [],
   "source": [
    "def save_progress(d):\n",
    "  os.makedirs(os.path.dirname(PROGRESS_JSON),exist_ok=True)\n",
    "  with open(PROGRESS_JSON,'w')as f:json.dump(d,f)\n",
    "  print(\"Progress saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4nOQuxbgAY8k"
   },
   "outputs": [],
   "source": [
    "\n",
    "def append_header_if_needed():\n",
    "    ex = os.path.exists(RAW_CSV)\n",
    "    os.makedirs(os.path.dirname(RAW_CSV), exist_ok=True)\n",
    "    if not ex:\n",
    "        with open(RAW_CSV, 'w', newline='', encoding='utf-8') as f:\n",
    "            # Note: 'channel_num' added to the header\n",
    "            csv.writer(f).writerow([\n",
    "                'channel', \n",
    "                'msg_id', \n",
    "                'date', \n",
    "                'text', \n",
    "                'fwd_from', \n",
    "                'fwd_id', \n",
    "                'bfs_cycle',\n",
    "                'channel_num'\n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HoqHVl86E-_U"
   },
   "outputs": [],
   "source": [
    "async def join_if_needed(ent):\n",
    "  if getattr(ent,'megagroup',False):\n",
    "    try:await client(JoinChannelRequest(ent))\n",
    "    except:pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5pQiuvbeE_Ax"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "#  scrape_channel – flood-wait-aware, entity-optional\n",
    "# --------------------------------------------------------------------\n",
    "async def scrape_channel(\n",
    "        name: str,\n",
    "        last_id: int,\n",
    "        bfs_depth: int,\n",
    "        channel_num: int,\n",
    "        *,\n",
    "        entity=None,\n",
    "        msg_limit: int | None = None\n",
    "    ) -> int:\n",
    "\n",
    "    if entity is None:                       # resolve if caller didn’t pass it\n",
    "        entity = await safe_get_entity(client, name)\n",
    "        if entity is None:\n",
    "            print(\"Skip\", name, \"— cannot resolve\")\n",
    "            return last_id\n",
    "        await asyncio.sleep(1.1)\n",
    "\n",
    "    await join_if_needed(entity)\n",
    "\n",
    "    max_id  = last_id\n",
    "    fetched = 0\n",
    "\n",
    "    try:\n",
    "        async for msg in _batched_iter_messages(entity, limit=msg_limit):\n",
    "            if msg.id <= last_id:\n",
    "                continue\n",
    "            if msg_limit and fetched >= msg_limit:\n",
    "                break\n",
    "            fetched += 1\n",
    "            max_id = max(max_id, msg.id)\n",
    "\n",
    "            # ---------- CSV row ----------\n",
    "            fw_title, fw_id = None, None\n",
    "            if msg.forward:\n",
    "                if msg.forward.chat:\n",
    "                    fw_title = msg.forward.chat.title or msg.forward.chat.username\n",
    "                    fw_id    = getattr(msg.forward.chat, 'id', None)\n",
    "                elif msg.forward.sender:\n",
    "                    fw_title = (msg.forward.sender.first_name or\n",
    "                                msg.forward.sender.username or 'Private')\n",
    "                    fw_id    = getattr(msg.forward.sender, 'id', None)\n",
    "\n",
    "            with open(RAW_CSV, \"a\", newline='', encoding=\"utf-8\") as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    name, msg.id, msg.date.isoformat(), msg.text or '',\n",
    "                    fw_title, fw_id, bfs_depth, channel_num\n",
    "                ])\n",
    "\n",
    "    except FloodWaitError as e:\n",
    "        print(\"FloodWait while reading\", name, \"→ stop (\", e.seconds, \"s )\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error reading\", name, e.__class__.__name__)\n",
    "\n",
    "    print(name, f\"+{max_id - last_id} msgs; up to\", max_id)\n",
    "    return max_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Fk_TBzdwE_G9"
   },
   "outputs": [],
   "source": [
    "def run_scrape():\n",
    "  asyncio.run(scrape_once())\n",
    "  print(\"Scraping done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EWBJBeXIE_Jy"
   },
   "outputs": [],
   "source": [
    "def deduplicate(messages):\n",
    "  out=[];seen=set()\n",
    "  for m in messages:\n",
    "    k=(m.get('fwd_id'),m.get('text'))\n",
    "    if m.get('fwd_id'):\n",
    "      if k in seen:continue\n",
    "      seen.add(k)\n",
    "    out.append(m)\n",
    "  return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "s2kMEf60E_MH"
   },
   "outputs": [],
   "source": [
    "def unshorten_url(u,t=5):\n",
    "  try:return requests.head(u,allow_redirects=True,timeout=t).url\n",
    "  except:return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "5S7R0ES3E_O1"
   },
   "outputs": [],
   "source": [
    "def expand_and_extract(m):\n",
    "  txt=m['text'];urls=re.findall(r'https?://\\S+',txt)\n",
    "  dom=set()\n",
    "  for u in urls:\n",
    "    fin=unshorten_url(u);d=urlparse(fin).netloc\n",
    "    dom.add(d)\n",
    "    txt=txt.replace(u,fin)\n",
    "  m['text']=txt;m['domains']=list(dom);return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "aFUAEBWiE_TT"
   },
   "outputs": [],
   "source": [
    "def is_english(txt):\n",
    "  if not txt.strip():return False\n",
    "  try:return(detect(txt)=='en')\n",
    "  except:return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xvy8iJ-HE_Vn"
   },
   "outputs": [],
   "source": [
    "def anonymize(m,user_map):\n",
    "  if m.get('fwd_id')and m['fwd_from']and not str(m['fwd_from']).startswith(('Channel','@')):\n",
    "    if m['fwd_id'] not in user_map:user_map[m['fwd_id']]=f\"User{len(user_map)+1}\"\n",
    "    m['fwd_from']=user_map[m['fwd_id']];m['fwd_id']=None\n",
    "  m['text']=re.sub(r'@[\\w\\d_]+','@USER',m['text'])\n",
    "  return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XuQE2tRRFPeo"
   },
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "  if not os.path.exists(RAW_CSV):print(\"No raw CSV.\");return\n",
    "  df=pd.read_csv(RAW_CSV,names=['chan','msg_id','date','text','fwd_from','fwd_id'],header=0)\n",
    "  msgs=df.to_dict('records');msgs=deduplicate(msgs)\n",
    "  out=[];umap={}\n",
    "  for m in msgs:\n",
    "    m=expand_and_extract(m)\n",
    "    if is_english(m['text']):m=anonymize(m,umap);out.append(m)\n",
    "  pd.DataFrame(out).to_csv(CLEAN_CSV,index=False)\n",
    "  print(f\"Saved {len(out)} cleaned msgs to {CLEAN_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Ir_L4HSPdya5"
   },
   "outputs": [],
   "source": [
    "# Load existing progress data and merge new seeds\n",
    "progress = load_progress()\n",
    "for username in seed_usernames:\n",
    "    if username not in progress:\n",
    "        progress[username] = 0  # mark as not yet scraped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Where should we store run-time artefacts?\n",
    "WORKDIR = Path(os.getenv(\"TG_WORKDIR\", Path.cwd() / \"telegram_data\"))\n",
    "WORKDIR.mkdir(parents=True, exist_ok=True)   # makes ./telegram_data if absent\n",
    "\n",
    "PROGRESS_JSON = WORKDIR / \"progress.json\"\n",
    "RAW_CSV       = WORKDIR / \"messages.csv\"\n",
    "CLEAN_CSV     = WORKDIR / \"cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "qocb8QV2dyg3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved.\n"
     ]
    }
   ],
   "source": [
    "# Persist updated progress\n",
    "save_progress(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "h8KS4Vdgv4Pu"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "async def _en_ratio(ent, sample=BFS_LANG_SAMPLE):\n",
    "    en = 0\n",
    "    async for m in client.iter_messages(ent, limit=sample):\n",
    "        t = (m.message or '').strip()\n",
    "        if t:\n",
    "            try:  en += detect(t)=='en'\n",
    "            except: pass\n",
    "    return 0 if not sample else en/sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "d5gZoB5XwXYm"
   },
   "outputs": [],
   "source": [
    "# Example : _forward_sources  (cell 33) ------------------------------\n",
    "async def _forward_sources(ent, limit, parent_hop):\n",
    "    \"\"\"\n",
    "    Scan recent messages for official forwards.\n",
    "    Returns a set of discovered numeric channel_ids and records them in `pending_ids`.\n",
    "    \"\"\"\n",
    "    global pending_ids\n",
    "    counts = {}\n",
    "\n",
    "    async for m in _batched_iter_messages(ent, limit=limit):\n",
    "        pc = m.forward.chat or m.forward.original_fwd if m.forward else None\n",
    "        if isinstance(pc, PeerChannel):\n",
    "            cid, ah = pc.channel_id, pc.access_hash\n",
    "            if cid and ah:\n",
    "                pending_ids.setdefault(cid, (ah, parent_hop))\n",
    "            counts[cid] = counts.get(cid, 0) + 1\n",
    "\n",
    "    return {cid for cid, c in counts.items() if c >= BFS_FWD_MIN}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bulk numeric-id → @username resolver ---------------------------------\n",
    "from telethon.tl.functions.channels import GetChannelsRequest\n",
    "from telethon.tl.types import InputChannel\n",
    "\n",
    "async def bulk_resolve_channels(client) -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Turn queued numeric ids in `pending_ids` into canonical '@handles'\n",
    "    in batches of ≤MAX_PER_CALL, obeying safe_api_call().\n",
    "    Returns list of tuples: (username, depth).\n",
    "    \"\"\"\n",
    "    global pending_ids\n",
    "    if not pending_ids:\n",
    "        return []\n",
    "\n",
    "    items = list(pending_ids.items())\n",
    "    pending_ids.clear()\n",
    "\n",
    "    resolved = []\n",
    "    for i in range(0, len(items), MAX_PER_CALL):\n",
    "        batch  = items[i:i+MAX_PER_CALL]\n",
    "        inputs = [InputChannel(cid, ah) for cid, (ah, _) in batch]\n",
    "\n",
    "        res = await safe_api_call(client, GetChannelsRequest(id=inputs))\n",
    "\n",
    "        depth_of = {cid: parent for cid, (_, parent) in batch}\n",
    "        for ch in res.chats:\n",
    "            if getattr(ch, \"username\", None):\n",
    "                uname = '@' + ch.username.lower()\n",
    "                resolved.append((uname, depth_of.get(ch.id, 0) + 1))\n",
    "\n",
    "        # courteous pause between big RPCs\n",
    "        await asyncio.sleep(BFS_PAUSE_SEC + random.random()*0.4)\n",
    "\n",
    "    return resolved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MENTION_RE = re.compile(r'@([\\w\\d_]{3,32})|t.me/([\\w\\d_]{3,32})', re.IGNORECASE)\n",
    "\n",
    "# Additional patterns for textual forward references\n",
    "FORWARDED_FROM_RE = re.compile(\n",
    "    r'(?:\\[?\\s*forwarded\\s+from\\s+([\\w\\d_.-]+)\\]?)',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# e.g. \"Forwarded from Disclose.tv\" or \"[Forwarded from MyChannel]\"\n",
    "# This is naive: we assume channel-like text is one token (no spaces).\n",
    "# If the channel doesn't actually exist, it won't pass _is_valid() anyway.\n",
    "EXT_REF_RE = re.compile(\n",
    "    r'(?:credit|source|via|thanks\\s+to|shared\\s+from|copied\\s+from)\\s*[:-]?\\s*@?([\\w\\d_]{3,32})',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# --- Cell 34  (REPLACE) ---------------------------------------------\n",
    "async def _mention_sources(ent, limit):\n",
    "    \"\"\"\n",
    "    Look for @mentions, t.me links, 'Forwarded from …', 'Source: …', etc.\n",
    "    Returns a set of @usernames (with leading '@').\n",
    "    \"\"\"\n",
    "    src = set()\n",
    "    if not BFS_INCLUDE_MENTIONS:\n",
    "        return src\n",
    "\n",
    "    async for m in _batched_iter_messages(ent, limit=limit):\n",
    "        text = m.message or \"\"\n",
    "\n",
    "        # classic @mention or t.me/…\n",
    "        for a, b in MENTION_RE.findall(text):\n",
    "            name = a or b\n",
    "            if name:\n",
    "                src.add(\"@\" + name.lstrip(\"@\"))\n",
    "\n",
    "        # \"Forwarded from Disclose.tv\"\n",
    "        for fwd in FORWARDED_FROM_RE.findall(text):\n",
    "            candidate = fwd.strip()\n",
    "            if \" \" not in candidate:\n",
    "                src.add(\"@\" + candidate.replace(\".\", \"\").lstrip(\"@\"))\n",
    "\n",
    "        # \"Source: @ChannelName\"\n",
    "        for e2 in EXT_REF_RE.findall(text):\n",
    "            src.add(\"@\" + e2.lstrip(\"@\"))\n",
    "\n",
    "    return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, asyncio\n",
    "TME_LINK_RE = re.compile(r\"(?:t\\.me/|@)([A-Za-z0-9_]{5,})\")\n",
    "\n",
    "async def _link_sources(ent, limit=100):\n",
    "    \"\"\"\n",
    "    Harvest plain-text @handles or t.me links from the last `limit` messages.\n",
    "    Returns a set of @usernames (with leading '@').\n",
    "    \"\"\"\n",
    "    src = set()\n",
    "    async for m in _batched_iter_messages(ent, limit=limit):\n",
    "        text = m.message or \"\"\n",
    "        for handle in TME_LINK_RE.findall(text):\n",
    "            src.add(\"@\" + handle.lstrip(\"@\"))\n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _to_username(peer):\n",
    "    \"\"\"\n",
    "    Convert any Telethon Peer / id / string into a canonical '@username'.\n",
    "    Returns None if it cannot be resolved.\n",
    "    \"\"\"\n",
    "    if isinstance(peer, str):\n",
    "        return '@' + peer.lstrip('@')\n",
    "\n",
    "    try:\n",
    "        ent = await safe_api_call(client.get_entity, peer)\n",
    "        uname = getattr(ent, 'username', None)\n",
    "        if uname:\n",
    "            return '@' + uname.lstrip('@')\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _topic_ratio(ent, keywords, sample):\n",
    "    \"\"\"\n",
    "    Checks what fraction of the last `sample` messages contain any of the keywords.\n",
    "    If keywords is empty, returns 1.0 to skip.\n",
    "    \"\"\"\n",
    "    if not keywords:\n",
    "        return 1.0\n",
    "    pat = re.compile('|'.join(re.escape(k) for k in keywords), re.I)\n",
    "    hits = 0\n",
    "    tot = 0\n",
    "    async for m in client.iter_messages(ent, limit=sample):\n",
    "        if BFS_PAUSE_SEC:\n",
    "            await asyncio.sleep(BFS_PAUSE_SEC)\n",
    "        text = (m.message or '')\n",
    "        if text.strip():\n",
    "            tot += 1\n",
    "            if pat.search(text):\n",
    "                hits += 1\n",
    "    return 0 if not tot else hits / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "eFK9klEFwXbm",
    "outputId": "f6a4f455-e91c-4f60-9956-8dd45ba63c5f"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "async def _is_valid(ent, *, debug=False):\n",
    "    \"\"\"Return (is_ok , reasons_list).\"\"\"\n",
    "    reasons = []\n",
    "\n",
    "    # 1. subscriber count\n",
    "    count = getattr(ent, \"participants_count\", 0) or getattr(ent, \"members_count\", 0)\n",
    "    if count < BFS_MIN_SUBS:\n",
    "        reasons.append(f\"subs={count} < {BFS_MIN_SUBS}\")\n",
    "\n",
    "    # 2. language ratio\n",
    "    en_ratio = await _en_ratio(ent)\n",
    "    if en_ratio < BFS_EN_RATIO:\n",
    "        reasons.append(f\"en_ratio={en_ratio:.2f} < {BFS_EN_RATIO}\")\n",
    "\n",
    "    # 3. whatever other tests you have …\n",
    "    # if …: reasons.append(\"some other test\")\n",
    "\n",
    "    ok = not reasons\n",
    "    if debug and reasons:\n",
    "        print(\"    ✖ REJECT\", getattr(ent, \"username\", \"?\"), \" | \".join(reasons), flush=True)\n",
    "\n",
    "    return ok, reasons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wjHw18VOwXeS"
   },
   "outputs": [],
   "source": [
    "async def snowball_discovery():\n",
    "    global CHANNELS\n",
    "    progress = load_progress()\n",
    "\n",
    "    # 1) Normalize existing CHANNELS so all have leading '@'\n",
    "    seeds = []\n",
    "    for c in CHANNELS:\n",
    "        c = c.strip()\n",
    "        if not c.startswith('@'):\n",
    "            c = '@' + c\n",
    "        seeds.append(c)\n",
    "\n",
    "    # 2) Discover BFS\n",
    "    validated, all_discovered, depth_map = await discover_bfs(seeds, max_hops=BFS_HOPS)\n",
    "\n",
    "    # 3) Mark validated channels in progress\n",
    "    for v in validated:\n",
    "        if v not in progress:\n",
    "            progress[v] = 0\n",
    "    save_progress(progress)\n",
    "\n",
    "    # 4) Merge newly validated channels, again force '@'\n",
    "    merged = set()\n",
    "    for ch in (list(CHANNELS) + list(validated)):\n",
    "        ch = ch.strip()\n",
    "        if not ch.startswith('@'):\n",
    "            ch = '@' + ch\n",
    "        merged.add(ch)\n",
    "\n",
    "    CHANNELS = list(merged)\n",
    "\n",
    "    # 5) Summary\n",
    "    print(\n",
    "        'discovered:', len(all_discovered),\n",
    "        '| validated:', len(validated),\n",
    "        '| total CHANNELS:', len(CHANNELS)\n",
    "    )\n",
    "\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "zOHfC_v0wXg5"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "#  scrape_once – rate-safe, flood-wait-aware version\n",
    "# --------------------------------------------------------------------\n",
    "from telethon.errors import FloodWaitError\n",
    "\n",
    "async def scrape_once():\n",
    "    \"\"\"Run one full cycle: BFS discovery ➜ per-channel scraping ➜ save progress.\"\"\"\n",
    "    await client.start(phone=PHONE)\n",
    "\n",
    "    # 1 ▸ Discover new channels and get their BFS depth\n",
    "    depth_map = await snowball_discovery()\n",
    "\n",
    "    # 2 ▸ Load previous scraping progress\n",
    "    last = load_progress()\n",
    "\n",
    "    # 3 ▸ Make sure the CSV header exists\n",
    "    append_header_if_needed()\n",
    "\n",
    "    # 4 ▸ Scrape each validated channel\n",
    "    total = len(CHANNELS)\n",
    "    for idx, handle in enumerate(CHANNELS, 1):\n",
    "        print(f\"[{idx}/{total}] Scraping channel: {handle}\")\n",
    "\n",
    "        # ---------- safe resolver (cache + flood-wait aware) ----------\n",
    "        ent = await safe_get_entity(client, handle)\n",
    "        if ent is None:                      # bad handle or 11h FloodWait\n",
    "            continue\n",
    "        await asyncio.sleep(1.1)            # keep resolve rate < 1 req/s\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        # depth of this handle in the discovery graph\n",
    "        bfs_depth = depth_map.get(handle, 0)\n",
    "\n",
    "        # ---------- download messages with its own flood-wait guard -----\n",
    "        try:\n",
    "            last[handle] = await scrape_channel(\n",
    "                name=handle,\n",
    "                last_id=last.get(handle, 0),\n",
    "                bfs_depth=bfs_depth,\n",
    "                channel_num=idx,\n",
    "                entity=ent                       # pass the resolved entity\n",
    "            )\n",
    "\n",
    "        except FloodWaitError as e:\n",
    "            if e.seconds > 600:                 # long wait → skip the channel\n",
    "                print(f\"Skip {handle} — FloodWait {e.seconds}s\")\n",
    "                continue\n",
    "            print(f\"Sleeping {e.seconds}s while reading {handle}\")\n",
    "            await asyncio.sleep(e.seconds + 2)\n",
    "            # one retry (optional)\n",
    "            last[handle] = await scrape_channel(\n",
    "                name=handle,\n",
    "                last_id=last.get(handle, 0),\n",
    "                bfs_depth=bfs_depth,\n",
    "                channel_num=idx,\n",
    "                entity=ent\n",
    "            )\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        if BFS_PAUSE_SEC:\n",
    "            await asyncio.sleep(BFS_PAUSE_SEC)\n",
    "\n",
    "    # 5 ▸ Persist progress and disconnect cleanly\n",
    "    save_progress(last)\n",
    "    await client.disconnect()\n",
    "# --------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Ly3Z1jugFPhg"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  run_scrape()\n",
    "  preprocess_data()\n",
    "  print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "#  Helper regexes, cache, safe_get_entity, and the patched discover_bfs()\n",
    "# =================================================================================\n",
    "import re, asyncio\n",
    "from collections import deque\n",
    "from telethon.errors import FloodWaitError\n",
    "from telethon.tl.types import Channel, Chat\n",
    "\n",
    "# ---------- reusable regexes -----------------------------------------------------\n",
    "VALID_UNAME  = re.compile(r'^@[A-Za-z0-9_]{5,32}$')             # canonical @handle\n",
    "TME_LINK_RE  = re.compile(r'(?:t\\.me/|@)([A-Za-z0-9_]{5,})')    # link extractor\n",
    "\n",
    "# ---------- @handle validator --------------------------------------------\n",
    "VALID_UNAME = re.compile(r'^@[A-Za-z0-9_]{5,32}$')\n",
    "def looks_like_username(s: str) -> bool:\n",
    "    return bool(VALID_UNAME.match(s.strip()))\n",
    "\n",
    "_entity_cache: dict[str, Channel | Chat] = {}\n",
    "\n",
    "async def safe_get_entity(client, handle: str):\n",
    "    \"\"\"\n",
    "    Cached @username → entity resolver that runs through safe_api_call.\n",
    "    Returns None on bad format, private/deleted channels, or long waits.\n",
    "    \"\"\"\n",
    "    handle = handle.strip()\n",
    "    if not looks_like_username(handle):\n",
    "        return None\n",
    "\n",
    "    if handle in _entity_cache:\n",
    "        return _entity_cache[handle]\n",
    "\n",
    "    try:\n",
    "        ent = await safe_api_call(client.get_entity, handle)\n",
    "        _entity_cache[handle] = ent\n",
    "        return ent\n",
    "    except Exception:\n",
    "        return None          # includes any FloodWait already handled upstream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "#  discover_bfs()  – numeric-ID aware, bulk-resolving version\n",
    "# =================================================================================\n",
    "async def discover_bfs(\n",
    "        seeds,\n",
    "        *,\n",
    "        max_hops: int = BFS_HOPS,\n",
    "        scanned_user_ids: set | None = None,    # for _participants_bounce\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Breadth-first discovery across Telegram channels / groups.\n",
    "    Returns (validated_set, all_discovered_set, depth_map).\n",
    "    \"\"\"\n",
    "    print(\"### ENTER discover_bfs ###\", flush=True)\n",
    "\n",
    "    # ---------- initial frontier --------------------------------------\n",
    "    visited        = set(seeds)\n",
    "    queue          = deque((s, 0) for s in seeds)   # (handle, hop)\n",
    "    validated      = set()\n",
    "    all_discovered = set(seeds)\n",
    "    depth_map      = {s: 0 for s in seeds}\n",
    "\n",
    "    if scanned_user_ids is None:\n",
    "        scanned_user_ids = set()\n",
    "\n",
    "    # ---------- BFS loop ----------------------------------------------\n",
    "    while queue:\n",
    "        print(\"--- TOP OF LOOP ---\", queue[0][0], flush=True)\n",
    "        print(\n",
    "            f\"    progress → validated={len(validated):>4} \"\n",
    "            f\"visited={len(visited):>4} queue={len(queue):>3}\",\n",
    "            flush=True\n",
    "        )\n",
    "\n",
    "        username, hop = queue.popleft()\n",
    "        if hop >= max_hops:\n",
    "            continue\n",
    "\n",
    "        # 1) entity lookup (cached, rate-safe)\n",
    "        ent = await safe_get_entity(client, username)\n",
    "        if ent is None:\n",
    "            continue\n",
    "\n",
    "        all_discovered.add(username)\n",
    "\n",
    "        # 2) quick type flags\n",
    "        is_channel = getattr(ent, \"broadcast\", False)\n",
    "        is_group   = getattr(ent, \"megagroup\", False) or isinstance(ent, Chat)\n",
    "\n",
    "        # 3) validation\n",
    "        is_ok, _ = await _is_valid(ent, debug=True)\n",
    "        if not is_ok:\n",
    "            continue\n",
    "        validated.add(username)\n",
    "\n",
    "        # 4) how many messages to scan\n",
    "        msg_limit = BFS_GROUP_MSG_LIMIT if is_group else BFS_CHANNEL_MSG_LIMIT\n",
    "\n",
    "        # 5) neighbour harvest ----------------------------------------------------\n",
    "        neighbors = set()\n",
    "\n",
    "        fwd_sources     = await _forward_sources(ent, msg_limit, hop)\n",
    "        mention_sources = await _mention_sources(ent, msg_limit)\n",
    "        link_text       = await _link_sources(ent, msg_limit)\n",
    "        linked          = await _linked_discussion(client, ent)\n",
    "        pinned_about    = await _links_in_about_and_pinned(client, ent)\n",
    "        invites         = await _invite_links_in_messages(client, ent, msg_limit)\n",
    "        inline_entities = await _parse_message_entities(client, ent, msg_limit)\n",
    "        button_links    = await _parse_button_links(client, ent, msg_limit)\n",
    "\n",
    "        neighbors |= fwd_sources | mention_sources | link_text | linked\n",
    "        neighbors |= pinned_about | invites | inline_entities | button_links\n",
    "\n",
    "        if is_group:\n",
    "            neighbors |= await _participants_bounce(\n",
    "                client, ent, scanned_user_ids, limit=msg_limit\n",
    "            )\n",
    "\n",
    "        # -- debug ------------------------------------------------------\n",
    "        print(\n",
    "            f\"[DBG] {username}: {len(neighbors)} raw neighbours \"\n",
    "            f\"({len(fwd_sources)} fwd, {len(mention_sources)} mention, \"\n",
    "            f\"{len(link_text)} textlink, {len(linked)} linked, \"\n",
    "            f\"{len(pinned_about)} about/pinned, {len(invites)} invites, \"\n",
    "            f\"{len(inline_entities)} entities, {len(button_links)} buttons)\",\n",
    "            flush=True\n",
    "        )\n",
    "\n",
    "        # 6) canonicalise neighbours → '@handle', enqueue unseen ones ----\n",
    "        neighbors_clean = set()\n",
    "\n",
    "        # (a) numeric IDs from fwd_sources were already stored in pending_ids;\n",
    "        #     we'll resolve them in bulk later, so skip resolving here.\n",
    "\n",
    "        # (b) everything else: try to resolve now (cheap path)\n",
    "        for nb in neighbors:\n",
    "            if isinstance(nb, int):           # pure numeric ID → skip\n",
    "                continue\n",
    "            ent_nb = await safe_get_entity(client, nb)\n",
    "            if ent_nb and ent_nb.username:\n",
    "                neighbors_clean.add(\"@\" + ent_nb.username.lower())\n",
    "\n",
    "        for neigh in neighbors_clean:\n",
    "            if neigh not in visited:\n",
    "                visited.add(neigh)\n",
    "                depth_map[neigh] = hop + 1\n",
    "                queue.append((neigh, hop + 1))\n",
    "                all_discovered.add(neigh)\n",
    "\n",
    "        # 7) if we have accumulated a lot of numeric IDs, resolve them now\n",
    "        if len(pending_ids) >= RESOLVE_INTERVAL:\n",
    "            newly = await bulk_resolve_channels(client)\n",
    "            for uname, dep in newly:\n",
    "                if uname not in visited:\n",
    "                    visited.add(uname)\n",
    "                    depth_map[uname] = dep\n",
    "                    queue.append((uname, dep))\n",
    "                    all_discovered.add(uname)\n",
    "\n",
    "    # ---------- flush whatever numeric IDs remain ----------------------\n",
    "    newly = await bulk_resolve_channels(client)\n",
    "    for uname, dep in newly:\n",
    "        if uname not in visited:\n",
    "            visited.add(uname)\n",
    "            depth_map[uname] = dep\n",
    "            # may exceed max_hops; they'll be crawled next run\n",
    "            all_discovered.add(uname)\n",
    "\n",
    "    # ---------- summary banner ----------------------------------------\n",
    "    print(\n",
    "        f\"### DONE: validated={len(validated)}  \"\n",
    "        f\"all_discovered={len(all_discovered)}  \"\n",
    "        f\"max_depth={max(depth_map.values())}\",\n",
    "        flush=True\n",
    "    )\n",
    "    return validated, all_discovered, depth_map\n",
    "# =================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "7dYlGjV_INa4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import asyncio\n",
    "\n",
    "from telethon.tl.functions.channels import GetFullChannelRequest\n",
    "from telethon.tl.functions.contacts import ResolveUsernameRequest\n",
    "from telethon.tl.types import (\n",
    "    MessageEntityTextUrl,\n",
    "    MessageEntityMention,\n",
    "    MessageEntityMentionName,\n",
    "    PeerChannel\n",
    ")\n",
    "from datetime import datetime  # if needed for dormancy checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _get_full_channel(client, ent):\n",
    "    \"\"\"\n",
    "    A small helper that wraps GetFullChannelRequest.\n",
    "    Returns full_chat data or None if something fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return await client(GetFullChannelRequest(ent))\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _linked_discussion(client, ent):\n",
    "    \"\"\"\n",
    "    Check if this channel has a linked discussion group (linked_chat_id).\n",
    "    Returns a set with the linked group's username if found, else empty set.\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "    full = await _get_full_channel(client, ent)\n",
    "    if full and full.full_chat.linked_chat_id:\n",
    "        try:\n",
    "            linked_ent = await client.get_entity(full.full_chat.linked_chat_id)\n",
    "            if getattr(linked_ent, 'username', None):\n",
    "                found.add(linked_ent.username)\n",
    "        except:\n",
    "            pass\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _links_in_about(client, ent):\n",
    "    \"\"\"\n",
    "    Extract any @usernames / t.me links from a channel's 'about' text.\n",
    "    Returns a set of discovered usernames.\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "    full = await _get_full_channel(client, ent)\n",
    "    if not full:\n",
    "        return found\n",
    "\n",
    "    about = (full.full_chat.about or \"\").strip()\n",
    "    # match @username\n",
    "    for m in USERNAME_RE.findall(about):\n",
    "        found.add(m)\n",
    "    # match t.me/... patterns\n",
    "    for m in TMELINK_RE.findall(about):\n",
    "        found.add(m.lstrip(\"+/\"))  # remove leading '+' or 'joinchat/'\n",
    "\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _links_in_pinned_message(client, ent):\n",
    "    \"\"\"\n",
    "    If the channel has a pinned message, scan it for @usernames or t.me links.\n",
    "    Returns a set of discovered usernames.\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "    full = await _get_full_channel(client, ent)\n",
    "    if not full:\n",
    "        return found\n",
    "\n",
    "    if full.full_chat.pinned_msg_id:\n",
    "        try:\n",
    "            pinned_msg = await client.get_messages(ent, ids=full.full_chat.pinned_msg_id)\n",
    "            pinned_text = (pinned_msg.message or \"\").strip()\n",
    "            # match @username\n",
    "            for m in USERNAME_RE.findall(pinned_text):\n",
    "                found.add(m)\n",
    "            # match t.me/...\n",
    "            for m in TMELINK_RE.findall(pinned_text):\n",
    "                found.add(m.lstrip(\"+/\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _links_in_about_and_pinned(client, ent):\n",
    "    \"\"\"\n",
    "    Aggregates results from _links_in_about + _links_in_pinned_message.\n",
    "    Returns a combined set of discovered usernames.\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "    about_links = await _links_in_about(client, ent)\n",
    "    pinned_links = await _links_in_pinned_message(client, ent)\n",
    "    found |= about_links\n",
    "    found |= pinned_links\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_invite_slugs(text):\n",
    "    \"\"\"\n",
    "    Finds all t.me/... patterns in a piece of text, ignoring leading +/ in the slug.\n",
    "    Returns a list of invite slugs (strings).\n",
    "    \"\"\"\n",
    "    slugs = []\n",
    "    for match in TMELINK_RE.findall(text):\n",
    "        slug = match.lstrip(\"+/\")\n",
    "        if slug:\n",
    "            slugs.append(slug)\n",
    "    return slugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _resolve_invite_slugs(client, slugs):\n",
    "    \"\"\"\n",
    "    Given a list of invite slugs, attempt to ResolveUsernameRequest for each.\n",
    "    Return a set of discovered channel usernames.\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "    for slug in slugs:\n",
    "        try:\n",
    "            r = await client(ResolveUsernameRequest(slug))\n",
    "            for ch in r.chats:\n",
    "                if getattr(ch, 'username', None):\n",
    "                    found.add(ch.username)\n",
    "        except:\n",
    "            pass\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _invite_links_in_messages(client, ent, limit=100000):\n",
    "    \"\"\"\n",
    "    Extract t.me/… slugs, resolve them in bulk, return discovered usernames.\n",
    "    Works over the last `limit` messages.\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "\n",
    "    async for m in _batched_iter_messages(ent, limit=limit):\n",
    "        text = m.message or \"\"\n",
    "        slugs = _extract_invite_slugs(text)\n",
    "        if not slugs:\n",
    "            continue\n",
    "        resolved = await _resolve_invite_slugs(client, slugs)\n",
    "        found |= resolved\n",
    "\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _check_text_url_entity(client, entity, message_text):\n",
    "    \"\"\"\n",
    "    If it's a MessageEntityTextUrl, extract the URL, see if it has t.me pattern, \n",
    "    attempt to resolve.\n",
    "    Returns a set of discovered usernames.\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "    if entity.url:\n",
    "        match = TMELINK_RE.search(entity.url)\n",
    "        if match:\n",
    "            slug = match.group(1).lstrip(\"+/\")\n",
    "            try:\n",
    "                r = await client(ResolveUsernameRequest(slug))\n",
    "                for ch in r.chats:\n",
    "                    if getattr(ch, 'username', None):\n",
    "                        found.add(ch.username)\n",
    "            except:\n",
    "                pass\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_mention_entity(entity, message_text):\n",
    "    \"\"\"\n",
    "    If it's a mention or mention-name, parse out the substring from the message.\n",
    "    Returns a set with the mention text (without '@').\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "    from telethon.tl.types import MessageEntityMention, MessageEntityMentionName\n",
    "    if isinstance(entity, (MessageEntityMention, MessageEntityMentionName)):\n",
    "        mention_str = message_text[entity.offset : entity.offset + entity.length]\n",
    "        mention_str = mention_str.lstrip('@')\n",
    "        if mention_str:\n",
    "            found.add(mention_str)\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def _parse_message_entities(client, ent, limit=100000):\n",
    "    \"\"\"\n",
    "    Walk through message entities (text-url, @mention, etc.).\n",
    "    Returns a set of discovered @usernames (without the leading '@').\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "\n",
    "    async for m in _batched_iter_messages(ent, limit=limit):\n",
    "        if not m.entities:\n",
    "            continue\n",
    "\n",
    "        txt = m.message or \"\"\n",
    "        for e in m.entities:\n",
    "            # 1) text-URL entities that contain a t.me link\n",
    "            if isinstance(e, MessageEntityTextUrl):\n",
    "                found |= await _check_text_url_entity(client, e, txt)\n",
    "            # 2) explicit mention / mention-name entities\n",
    "            else:\n",
    "                found |= _check_mention_entity(e, txt)\n",
    "\n",
    "    return found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def _parse_button_links(client, ent, limit=100000):\n",
    "    \"\"\"\n",
    "    Scan inline keyboard buttons for t.me links; resolve and return usernames.\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "\n",
    "    async for m in _batched_iter_messages(ent, limit=limit):\n",
    "        if not (m.reply_markup and getattr(m.reply_markup, \"inline_keyboard\", None)):\n",
    "            continue\n",
    "\n",
    "        for row in m.reply_markup.inline_keyboard:\n",
    "            for button in row:\n",
    "                if not button.url:\n",
    "                    continue\n",
    "                match = TMELINK_RE.search(button.url)\n",
    "                if not match:\n",
    "                    continue\n",
    "                slug = match.group(1).lstrip(\"+/\")\n",
    "                try:\n",
    "                    r = await client(ResolveUsernameRequest(slug))\n",
    "                    for ch in r.chats:\n",
    "                        if getattr(ch, \"username\", None):\n",
    "                            found.add(ch.username)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    return found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 51 (code)\n",
    "\n",
    "async def _participants_bounce(client, ent, scanned_user_ids, limit=200):\n",
    "    \"\"\"\n",
    "    If 'ent' is a group-type entity (megagroup), fetch participants (up to 'limit').\n",
    "    Then for each participant, scan their \"about\" text and their common chats \n",
    "    to discover potential public channels. We skip any user we've already scanned\n",
    "    (via scanned_user_ids) to reduce duplicate calls.\n",
    "    \n",
    "    Returns a set() of discovered channel or group usernames.\n",
    "    \"\"\"\n",
    "    found = set()\n",
    "\n",
    "    # Check if it's a megagroup or group\n",
    "    if hasattr(ent, 'megagroup') and ent.megagroup:\n",
    "        from telethon.tl.functions.channels import GetParticipantsRequest\n",
    "        from telethon.tl.types import ChannelParticipantsSearch\n",
    "\n",
    "        try:\n",
    "            offset = 0\n",
    "            batch_size = 200  # Telethon's limit per request\n",
    "            while True:\n",
    "                result = await client(GetParticipantsRequest(\n",
    "                    channel=ent,\n",
    "                    filter=ChannelParticipantsSearch(''),\n",
    "                    offset=offset,\n",
    "                    limit=min(batch_size, limit),\n",
    "                    hash=0\n",
    "                ))\n",
    "                users = result.users\n",
    "                if not users:\n",
    "                    break\n",
    "\n",
    "                for u in users:\n",
    "                    # Skip if we've scanned this user before\n",
    "                    if u.id in scanned_user_ids:\n",
    "                        continue\n",
    "                    scanned_user_ids.add(u.id)\n",
    "\n",
    "                    user_found = await _scan_user_for_channels(client, u)\n",
    "                    found |= user_found\n",
    "\n",
    "                offset += len(users)\n",
    "                if len(users) < batch_size:\n",
    "                    # No more participants to fetch or reached limit\n",
    "                    break\n",
    "\n",
    "                # If limit is very large, we might reduce it in steps\n",
    "                limit -= len(users)\n",
    "                if limit <= 0:\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"participants_bounce failed:\", e)\n",
    "\n",
    "    return found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "from telethon.tl.functions.users import GetFullUserRequest\n",
    "from telethon.tl.types import Channel, Chat\n",
    "\n",
    "async def _scan_user_for_channels(client, user_obj, pause_sec=0):\n",
    "    \"\"\"\n",
    "    Given a Telethon 'User' object, fetch their FullUser data,\n",
    "    then parse:\n",
    "      1) The user's 'about' text for @usernames or t.me links\n",
    "      2) The user's common chats for any channel/group that has a public username.\n",
    "    \n",
    "    Returns a set of discovered channel/group usernames (strings).\n",
    "    \"\"\"\n",
    "\n",
    "    found = set()\n",
    "\n",
    "    # Attempt to get full user info\n",
    "    try:\n",
    "        full_u = await client(GetFullUserRequest(user_obj))\n",
    "    except:\n",
    "        return found  # if we can't fetch the user, skip\n",
    "\n",
    "    # (A) Parse 'about' for potential links\n",
    "    about_text = (full_u.full_user.about or \"\").strip()\n",
    "    USERNAME_RE = re.compile(r'@([\\w\\d_]{5,32})', re.IGNORECASE)\n",
    "    TMELINK_RE  = re.compile(r'(?:https?://)?t\\.me/([\\w\\d_+/]{5,64})', re.IGNORECASE)\n",
    "\n",
    "    # 1. match @username\n",
    "    for match in USERNAME_RE.findall(about_text):\n",
    "        found.add(match)\n",
    "\n",
    "    # 2. match t.me/...\n",
    "    for match in TMELINK_RE.findall(about_text):\n",
    "        found.add(match.lstrip(\"+/\"))\n",
    "\n",
    "    # (B) Parse common chats from 'full_u.chats'\n",
    "    common_chats = getattr(full_u, 'chats', [])\n",
    "    for c in common_chats:\n",
    "        # c might be a Channel, Chat, or even a User\n",
    "        if isinstance(c, (Channel, Chat)):\n",
    "            uname = getattr(c, 'username', None)\n",
    "            if uname:\n",
    "                found.add(uname)\n",
    "\n",
    "    # pause to avoid spamming the API\n",
    "    if pause_sec:\n",
    "        import asyncio\n",
    "        await asyncio.sleep(pause_sec)\n",
    "\n",
    "    return found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGndW8SXFPkE"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the code you received:  76978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed in successfully as Yipeng Wang; remember to not break the ToS or you will risk an account ban!\n",
      "### ENTER discover_bfs ###\n",
      "--- TOP OF LOOP --- @BigPumpsBinance\n",
      "    progress → validated=   0 visited=  75 queue= 75\n",
      "[DBG] @BigPumpsBinance: 2 raw neighbours (0 fwd, 1 mention, 1 textlink, 0 linked, 1 about/pinned, 0 invites, 0 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @CoinCodeCap\n",
      "    progress → validated=   1 visited=  76 queue= 75\n",
      "[DBG] @CoinCodeCap: 15 raw neighbours (0 fwd, 4 mention, 2 textlink, 1 linked, 2 about/pinned, 0 invites, 9 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @UniversalCryptoSignals\n",
      "    progress → validated=   2 visited=  79 queue= 77\n",
      "[DBG] @UniversalCryptoSignals: 16 raw neighbours (0 fwd, 9 mention, 6 textlink, 0 linked, 4 about/pinned, 1 invites, 6 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @VerifiedCryptoNews\n",
      "    progress → validated=   3 visited=  84 queue= 81\n",
      "[DBG] @VerifiedCryptoNews: 18 raw neighbours (0 fwd, 2 mention, 2 textlink, 0 linked, 3 about/pinned, 0 invites, 14 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @AltSignals\n",
      "    progress → validated=   4 visited=  85 queue= 81\n",
      "[DBG] @AltSignals: 4 raw neighbours (0 fwd, 3 mention, 0 textlink, 0 linked, 1 about/pinned, 0 invites, 0 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @FatPigSignals\n",
      "    progress → validated=   5 visited=  85 queue= 80\n",
      "[DBG] @FatPigSignals: 52 raw neighbours (0 fwd, 5 mention, 4 textlink, 0 linked, 3 about/pinned, 1 invites, 46 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @RavenSignalsPro\n",
      "    progress → validated=   6 visited=  88 queue= 82\n",
      "[DBG] @RavenSignalsPro: 6 raw neighbours (0 fwd, 2 mention, 2 textlink, 0 linked, 2 about/pinned, 1 invites, 2 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @BinanceKillers\n",
      "    progress → validated=   7 visited=  89 queue= 82\n",
      "[DBG] @BinanceKillers: 7 raw neighbours (0 fwd, 3 mention, 2 textlink, 0 linked, 2 about/pinned, 0 invites, 3 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @AlphaTradeZone\n",
      "    progress → validated=   8 visited=  90 queue= 82\n",
      "[DBG] @AlphaTradeZone: 7 raw neighbours (0 fwd, 3 mention, 0 textlink, 1 linked, 2 about/pinned, 0 invites, 2 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @BullDogSignals\n",
      "    progress → validated=   9 visited=  91 queue= 82\n",
      "    ✖ REJECT BullDogSignals en_ratio=0.19 < 0.2\n",
      "--- TOP OF LOOP --- @ForsageIO_Official\n",
      "    progress → validated=   9 visited=  91 queue= 81\n",
      "[DBG] @ForsageIO_Official: 21 raw neighbours (0 fwd, 10 mention, 6 textlink, 1 linked, 1 about/pinned, 2 invites, 10 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @NovaTechFXOfficial\n",
      "    progress → validated=  10 visited=  95 queue= 84\n",
      "    ✖ REJECT novatechfxofficial en_ratio=0.18 < 0.2\n",
      "--- TOP OF LOOP --- @Learn2Trade\n",
      "    progress → validated=  10 visited=  95 queue= 83\n",
      "    ✖ REJECT None en_ratio=0.12 < 0.2\n",
      "--- TOP OF LOOP --- @PrimeTradingSignals\n",
      "    progress → validated=  10 visited=  95 queue= 82\n",
      "    ✖ REJECT PRIMETRADINGSIGNALS en_ratio=0.04 < 0.2\n",
      "--- TOP OF LOOP --- @WolfXSignals\n",
      "    progress → validated=  10 visited=  95 queue= 81\n",
      "    ✖ REJECT wolfxSignals en_ratio=0.07 < 0.2\n",
      "--- TOP OF LOOP --- @CryptoClassics\n",
      "    progress → validated=  10 visited=  95 queue= 80\n",
      "[DBG] @CryptoClassics: 15 raw neighbours (0 fwd, 7 mention, 4 textlink, 0 linked, 1 about/pinned, 1 invites, 8 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @CryptoMiami\n",
      "    progress → validated=  11 visited=  97 queue= 81\n",
      "    ✖ REJECT cryptomiami en_ratio=0.17 < 0.2\n",
      "--- TOP OF LOOP --- @CovidVaccineVictims\n",
      "    progress → validated=  11 visited=  97 queue= 80\n",
      "    ✖ REJECT covidvaccinevictims en_ratio=0.20 < 0.2\n",
      "--- TOP OF LOOP --- @ChildrensHD\n",
      "    progress → validated=  11 visited=  97 queue= 79\n",
      "[DBG] @ChildrensHD: 8 raw neighbours (0 fwd, 8 mention, 0 textlink, 0 linked, 0 about/pinned, 0 invites, 0 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @DrSimoneGold\n",
      "    progress → validated=  12 visited=  98 queue= 79\n",
      "[DBG] @DrSimoneGold: 21 raw neighbours (0 fwd, 9 mention, 8 textlink, 1 linked, 1 about/pinned, 5 invites, 8 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @AFLDSorg\n",
      "    progress → validated=  13 visited= 103 queue= 83\n",
      "    ✖ REJECT AFLDSOrg en_ratio=0.17 < 0.2\n",
      "--- TOP OF LOOP --- @BusyDrT\n",
      "    progress → validated=  13 visited= 103 queue= 82\n",
      "    ✖ REJECT BusyDrT en_ratio=0.05 < 0.2\n",
      "--- TOP OF LOOP --- @JudyAMikovits\n",
      "    progress → validated=  13 visited= 103 queue= 81\n",
      "[DBG] @JudyAMikovits: 17 raw neighbours (0 fwd, 11 mention, 5 textlink, 0 linked, 0 about/pinned, 4 invites, 5 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @ZFreedomFoundation\n",
      "    progress → validated=  14 visited= 109 queue= 86\n",
      "    ✖ REJECT ZFreedomFoundation en_ratio=0.02 < 0.2\n",
      "--- TOP OF LOOP --- @NaturalNewsMedia\n",
      "    progress → validated=  14 visited= 109 queue= 85\n",
      "[DBG] @NaturalNewsMedia: 44 raw neighbours (0 fwd, 37 mention, 5 textlink, 0 linked, 1 about/pinned, 5 invites, 6 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @Pierre_Kory_Official\n",
      "    progress → validated=  15 visited= 118 queue= 93\n",
      "[DBG] @Pierre_Kory_Official: 38 raw neighbours (0 fwd, 25 mention, 19 textlink, 0 linked, 0 about/pinned, 12 invites, 2 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @DrDMartinWorld\n",
      "    progress → validated=  16 visited= 130 queue=104\n",
      "    ✖ REJECT DrDMartinWorld en_ratio=0.01 < 0.2\n",
      "--- TOP OF LOOP --- @Dr_EricBergDC\n",
      "    progress → validated=  16 visited= 130 queue=103\n",
      "[DBG] @Dr_EricBergDC: 13 raw neighbours (0 fwd, 3 mention, 2 textlink, 0 linked, 0 about/pinned, 2 invites, 10 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @DoctorMercola\n",
      "    progress → validated=  17 visited= 132 queue=104\n",
      "[DBG] @DoctorMercola: 2 raw neighbours (0 fwd, 2 mention, 0 textlink, 0 linked, 0 about/pinned, 0 invites, 0 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @DrMikeYeadon\n",
      "    progress → validated=  18 visited= 132 queue=103\n",
      "[DBG] @DrMikeYeadon: 28 raw neighbours (0 fwd, 23 mention, 11 textlink, 0 linked, 0 about/pinned, 4 invites, 3 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @WorldDoctorsAlliance\n",
      "    progress → validated=  19 visited= 139 queue=109\n",
      "[DBG] @WorldDoctorsAlliance: 101 raw neighbours (0 fwd, 54 mention, 45 textlink, 0 linked, 0 about/pinned, 30 invites, 32 entities, 0 buttons)\n",
      "⚠️  FloodWait 84695s → sleeping 84697s …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [Errno 60] Operation timed out\n",
      "Graceful disconnection timed out, forcibly ignoring cleanup\n",
      "Server closed the connection: [Errno 54] Connection reset by peer\n",
      "Server closed the connection: [Errno 54] Connection reset by peer\n",
      "Server resent the older message 7509554276996722689, ignoring\n",
      "Server closed the connection: [Errno 54] Connection reset by peer\n",
      "Server closed the connection: [Errno 54] Connection reset by peer\n",
      "Server resent the older message 7509576103950749697, ignoring\n",
      "Attempt 1 at connecting failed: OSError: [Errno 51] Network is unreachable\n",
      "Attempt 2 at connecting failed: OSError: [Errno 51] Network is unreachable\n",
      "Attempt 3 at connecting failed: OSError: [Errno 51] Network is unreachable\n",
      "Attempt 4 at connecting failed: OSError: [Errno 51] Network is unreachable\n",
      "Attempt 5 at connecting failed: OSError: [Errno 51] Network is unreachable\n",
      "Attempt 6 at connecting failed: OSError: [Errno 51] Network is unreachable\n",
      "Server closed the connection: [Errno 60] Operation timed out\n",
      "Graceful disconnection timed out, forcibly ignoring cleanup\n",
      "Server resent the older message 7509611653765012481, ignoring\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TOP OF LOOP --- @TheTruthAboutCancer_Vaccines\n",
      "    progress → validated=  20 visited= 176 queue=145\n",
      "[DBG] @TheTruthAboutCancer_Vaccines: 32 raw neighbours (0 fwd, 14 mention, 11 textlink, 0 linked, 1 about/pinned, 7 invites, 13 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @DrJaneRuby\n",
      "    progress → validated=  21 visited= 182 queue=150\n",
      "[DBG] @DrJaneRuby: 115 raw neighbours (0 fwd, 55 mention, 51 textlink, 1 linked, 2 about/pinned, 19 invites, 53 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @PeterMcCullough\n",
      "    progress → validated=  22 visited= 204 queue=171\n",
      "[DBG] @PeterMcCullough: 11 raw neighbours (0 fwd, 5 mention, 4 textlink, 0 linked, 1 about/pinned, 3 invites, 4 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @FLCCC_Alliance\n",
      "    progress → validated=  23 visited= 205 queue=171\n",
      "    ✖ REJECT FLCCC_Alliance en_ratio=0.01 < 0.2\n",
      "--- TOP OF LOOP --- @VigilantFox\n",
      "    progress → validated=  23 visited= 205 queue=170\n",
      "[DBG] @VigilantFox: 79 raw neighbours (0 fwd, 37 mention, 34 textlink, 1 linked, 2 about/pinned, 20 invites, 37 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @CryptoSignalsorg\n",
      "    progress → validated=  24 visited= 222 queue=186\n",
      "--- TOP OF LOOP --- @OnwardBTC\n",
      "    progress → validated=  24 visited= 222 queue=185\n",
      "    ✖ REJECT onwardbtc en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @MedBedTech\n",
      "    progress → validated=  24 visited= 222 queue=184\n",
      "--- TOP OF LOOP --- @UnitedKingsSignals\n",
      "    progress → validated=  24 visited= 222 queue=183\n",
      "    ✖ REJECT UnitedKingsSignals en_ratio=0.17 < 0.2\n",
      "--- TOP OF LOOP --- @TopTradingSignals\n",
      "    progress → validated=  24 visited= 222 queue=182\n",
      "    ✖ REJECT toptradingsignals en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @VasilyTrader\n",
      "    progress → validated=  24 visited= 222 queue=181\n",
      "    ✖ REJECT VasilyTrader en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @prayingmedic\n",
      "    progress → validated=  24 visited= 222 queue=180\n",
      "    ✖ REJECT prayingmedic en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @RealHealthRanger\n",
      "    progress → validated=  24 visited= 222 queue=179\n",
      "[DBG] @RealHealthRanger: 26 raw neighbours (0 fwd, 18 mention, 7 textlink, 0 linked, 0 about/pinned, 1 invites, 7 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @FixedMatchesTipsVIP\n",
      "    progress → validated=  25 visited= 227 queue=183\n",
      "--- TOP OF LOOP --- @CorrectScore_CS\n",
      "    progress → validated=  25 visited= 227 queue=182\n",
      "--- TOP OF LOOP --- @BinaryKingsVIP\n",
      "    progress → validated=  25 visited= 227 queue=181\n",
      "--- TOP OF LOOP --- @FXSniperSignals\n",
      "    progress → validated=  25 visited= 227 queue=180\n",
      "--- TOP OF LOOP --- @GoldSignalsVIP\n",
      "    progress → validated=  25 visited= 227 queue=179\n",
      "[DBG] @GoldSignalsVIP: 5 raw neighbours (0 fwd, 3 mention, 1 textlink, 0 linked, 1 about/pinned, 0 invites, 2 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @forexgoldkingsignals\n",
      "    progress → validated=  26 visited= 228 queue=179\n",
      "[DBG] @forexgoldkingsignals: 2 raw neighbours (0 fwd, 2 mention, 0 textlink, 0 linked, 0 about/pinned, 0 invites, 0 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @SureShotFX\n",
      "    progress → validated=  27 visited= 228 queue=178\n",
      "--- TOP OF LOOP --- @ApexBull\n",
      "    progress → validated=  27 visited= 228 queue=177\n",
      "    ✖ REJECT apexbull en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @GoldSignalsio\n",
      "    progress → validated=  27 visited= 228 queue=176\n",
      "--- TOP OF LOOP --- @ForexSignalsio\n",
      "    progress → validated=  27 visited= 228 queue=175\n",
      "--- TOP OF LOOP --- @CryptoClubPump\n",
      "    progress → validated=  27 visited= 228 queue=174\n",
      "    ✖ REJECT cryptoclubpump en_ratio=0.02 < 0.2\n",
      "--- TOP OF LOOP --- @PUMPNOW800\n",
      "    progress → validated=  27 visited= 228 queue=173\n",
      "--- TOP OF LOOP --- @insidebetscrypto\n",
      "    progress → validated=  27 visited= 228 queue=172\n",
      "    ✖ REJECT insidebetscrypto en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @BullstarSignals\n",
      "    progress → validated=  27 visited= 228 queue=171\n",
      "    ✖ REJECT Bullstarsignals en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @binancesignals\n",
      "    progress → validated=  27 visited= 228 queue=170\n",
      "[DBG] @binancesignals: 8 raw neighbours (0 fwd, 4 mention, 2 textlink, 0 linked, 2 about/pinned, 0 invites, 2 entities, 0 buttons)\n",
      "--- TOP OF LOOP --- @degeninvestor\n",
      "    progress → validated=  28 visited= 231 queue=172\n",
      "    ✖ REJECT degeninvestor en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @Rose_Javelin\n",
      "    progress → validated=  28 visited= 231 queue=171\n",
      "    ✖ REJECT Rose_Javelin en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @elibraX\n",
      "    progress → validated=  28 visited= 231 queue=170\n",
      "    ✖ REJECT elibraX en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @DaveCF\n",
      "    progress → validated=  28 visited= 231 queue=169\n",
      "    ✖ REJECT DAVECF en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @worldlibertyfi\n",
      "    progress → validated=  28 visited= 231 queue=168\n",
      "    ✖ REJECT WorldLibertyfi en_ratio=0.00 < 0.2\n",
      "--- TOP OF LOOP --- @officialomegapro\n",
      "    progress → validated=  28 visited= 231 queue=167\n",
      "--- TOP OF LOOP --- @mining_city_official\n",
      "    progress → validated=  28 visited= 231 queue=166\n",
      "⚠️  FloodWait 85446s → sleeping 85448s …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [Errno 54] Connection reset by peer\n",
      "Server resent the older message 7509996555889563649, ignoring\n",
      "Server closed the connection: [Errno 54] Connection reset by peer\n",
      "Server closed the connection: [Errno 54] Connection reset by peer\n",
      "Server closed the connection: [Errno 54] Connection reset by peer\n",
      "Server closed the connection: [Errno 54] Connection reset by peer\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
